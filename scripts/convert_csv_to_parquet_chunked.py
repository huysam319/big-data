#!/usr/bin/env python3
"""
Robust CSV.gz to Parquet Converter (chunked processing, row-skip on errors)

- Äá»c theo chunks vá»›i on_bad_lines="skip" Ä‘á»ƒ bá» cÃ¡c dÃ²ng CSV sai Ä‘á»‹nh dáº¡ng.
- CÄƒn kiá»ƒu theo schema cá»§a chunk Ä‘áº§u tiÃªn cho cÃ¡c chunk sau.
- Náº¿u cáº£ chunk váº«n lá»—i khi ghi (PyArrow), dÃ¹ng chiáº¿n lÆ°á»£c "bisect" Ä‘á»ƒ loáº¡i Ä‘Ãºng
  cÃ¡c hÃ ng gÃ¢y lá»—i vÃ  váº«n ghi cÃ¡c hÃ ng cÃ²n láº¡i.

Usage:
    python convert_csv_to_parquet_chunked.py <input_csv.gz> <output_parquet>
    python convert_csv_to_parquet_chunked.py --input <input_csv.gz> --output <output_parquet>
"""

import argparse
import os
import sys
from pathlib import Path
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import numpy as np

# --------------------- Helpers ---------------------

def validate_file_path(file_path, file_type="file"):
    path = Path(file_path)
    if file_type == "input":
        if not path.exists():
            raise FileNotFoundError(f"Input file does not exist: {file_path}")
        if not path.is_file():
            raise ValueError(f"Input path is not a file: {file_path}")
        if not os.access(file_path, os.R_OK):
            raise PermissionError(f"No read permission for file: {file_path}")
    elif file_type == "output":
        outdir = path.parent
        if not outdir.exists():
            try:
                outdir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                raise PermissionError(f"Cannot create output directory: {e}")
        if not os.access(outdir, os.W_OK):
            raise PermissionError(f"No write permission for directory: {outdir}")

def normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Chuáº©n hÃ³a chunk Ä‘áº§u tiÃªn Ä‘á»ƒ suy ra schema há»£p lÃ½."""
    out = df.copy()
    for c in out.columns:
        # Äáº§u tiÃªn Ã©p vá» string dtype (nullable) Ä‘á»ƒ trÃ¡nh object há»—n táº¡p
        out[c] = out[c].astype("string")

        # Chuáº©n hÃ³a cÃ¡c giÃ¡ trá»‹ null phá»• biáº¿n
        out[c] = out[c].replace(
            ['nan', 'None', 'NULL', 'null', 'N/A', 'n/a', '', ' '],
            pd.NA
        )

        # Heuristic: náº¿u Ä‘a sá»‘ lÃ  sá»‘ -> chuyá»ƒn sá»‘
        try:
            num = pd.to_numeric(out[c], errors="coerce")
            if num.notna().mean() >= 0.7:
                # DÃ¹ng kiá»ƒu sá»‘ nullable Ä‘á»ƒ váº«n cháº¥p nháº­n NA
                if (num.dropna() % 1 == 0).all():
                    out[c] = num.astype("Int64")
                else:
                    out[c] = num.astype("Float64")
        except Exception:
            pass
    return out

def coerce_to_schema(df: pd.DataFrame, schema: pa.Schema) -> pd.DataFrame:
    """CÄƒn DataFrame theo schema Ä‘Ã£ cÃ³ Ä‘á»ƒ giáº£m kháº£ nÄƒng lá»—i khi viáº¿t."""
    out = df.copy()

    # Bá»• sung cá»™t thiáº¿u + sáº¯p xáº¿p cá»™t Ä‘Ãºng thá»© tá»± schema
    for name in schema.names:
        if name not in out.columns:
            out[name] = pd.NA
    out = out[schema.names]

    # Chuyá»ƒn kiá»ƒu tá»«ng cá»™t theo schema Arrow
    for field in schema:
        name = field.name
        atype = field.type

        col = out[name]

        # Báº¯t Ä‘áº§u tá»« dtype chuá»—i nullable Ä‘á»ƒ gá»n
        if not pd.api.types.is_string_dtype(col) and not pd.api.types.is_numeric_dtype(col):
            col = col.astype("string")

        try:
            if pa.types.is_integer(atype):
                col = pd.to_numeric(col, errors="coerce").astype("Int64")
            elif pa.types.is_floating(atype):
                col = pd.to_numeric(col, errors="coerce").astype("Float64")
            elif pa.types.is_boolean(atype):
                # Chuáº©n hÃ³a boolean tá»« cÃ¡c dáº¡ng phá»• biáº¿n
                mapping = {
                    "true": True, "false": False,
                    "1": True, "0": False,
                    "yes": True, "no": False,
                    "y": True, "n": False
                }
                s = col.astype("string").str.strip().str.lower()
                col = s.map(mapping).astype("boolean")
            elif pa.types.is_timestamp(atype):
                col = pd.to_datetime(col, errors="coerce")
            else:
                # Máº·c Ä‘á»‹nh string nullable
                col = col.astype("string").replace(
                    ['nan', 'None', 'NULL', 'null', 'N/A', 'n/a', '', ' '],
                    pd.NA
                )
        except Exception:
            # Náº¿u Ã©p kiá»ƒu tháº¥t báº¡i, rÆ¡i vá» string nullable
            col = col.astype("string")

        out[name] = col

    return out

def table_from_df_safe(df: pd.DataFrame, schema: pa.Schema):
    """
    Cá»‘ gáº¯ng táº¡o pa.Table tá»« df theo schema.
    Náº¿u tháº¥t báº¡i, tráº£ vá» None vÃ  phÃ¡t sinh exception Ä‘á»ƒ caller xá»­ lÃ½.
    """
    # preserve_index=False Ä‘á»ƒ khÃ´ng sinh cá»™t index
    return pa.Table.from_pandas(df, schema=schema, preserve_index=False, safe=False)

def bisect_write(writer: pq.ParquetWriter, df: pd.DataFrame, schema: pa.Schema) -> int:
    """
    Ghi 'an toÃ n' má»™t DataFrame:
    - Náº¿u ghi cáº£ khá»‘i thÃ nh cÃ´ng => ghi luÃ´n.
    - Náº¿u lá»—i => tÃ¡ch Ä‘Ã´i vÃ  thá»­ láº¡i tá»«ng ná»­a.
    - HÃ ng nÃ o khÃ´ng thá»ƒ ghi sáº½ bá»‹ loáº¡i.
    Tráº£ vá» sá»‘ dÃ²ng Ä‘Ã£ ghi.
    """
    if df.empty:
        return 0
    try:
        tbl = table_from_df_safe(df, schema)
        writer.write_table(tbl)
        return len(df)
    except Exception:
        # Náº¿u chá»‰ cÃ²n 1 hÃ ng mÃ  váº«n lá»—i => loáº¡i bá» hÃ ng Ä‘Ã³
        if len(df) == 1:
            return 0
        mid = len(df) // 2
        left = df.iloc[:mid]
        right = df.iloc[mid:]
        written_left = bisect_write(writer, left, schema)
        written_right = bisect_write(writer, right, schema)
        return written_left + written_right

def make_chunk_reader(input_path: str, chunk_size: int):
    """
    Táº¡o reader theo chunks, tá»± Ä‘á»™ng bá» dÃ²ng lá»—i.
    DÃ¹ng on_bad_lines='skip'. Náº¿u phiÃªn báº£n pandas cÅ©, fallback error_bad_lines=False.
    """
    base_kwargs = dict(
        compression="gzip",
        chunksize=chunk_size,
        dtype=str,
        na_values=['', 'NULL', 'null', 'None', 'N/A', 'n/a'],
        keep_default_na=True,
        low_memory=False,
        encoding="utf-8",
    )
    # encoding_errors cho pandas >= 1.5
    try:
        return pd.read_csv(input_path, on_bad_lines="skip", encoding_errors="replace", **base_kwargs)
    except TypeError:
        # pandas cÅ© khÃ´ng cÃ³ encoding_errors/on_bad_lines
        base_kwargs.pop("encoding", None)  # sáº½ Ä‘á»ƒ pandas tá»± suy Ä‘oÃ¡n náº¿u cáº§n
        return pd.read_csv(input_path, error_bad_lines=False, warn_bad_lines=True, **base_kwargs)

# --------------------- Core ---------------------

def convert_csv_to_parquet_chunked(input_path, output_path, chunk_size=50000, max_rows=10000000):
    try:
        validate_file_path(input_path, "input")
        validate_file_path(output_path, "output")

        # Náº¿u giá»›i háº¡n <= 0 â†’ táº¡o Parquet rá»—ng
        if max_rows is not None and max_rows <= 0:
            pq.write_table(pa.table({}), output_path, compression="snappy")
            print("âœ… Max rows = 0, created empty Parquet.")
            return True

        rows_budget = max_rows  # cÃ²n bao nhiÃªu hÃ ng Ä‘Æ°á»£c phÃ©p ghi
        print(f"Converting {input_path} to {output_path}...")
        print("ğŸ“¦ Äá»c CSV.gz theo tá»«ng chunk...")
        reader = make_chunk_reader(input_path, chunk_size)

        # Chunk Ä‘áº§u Ä‘á»ƒ suy schema
        try:
            first_chunk_raw = next(reader)
        except StopIteration:
            print("âš ï¸  File trá»‘ng. Táº¡o Parquet rá»—ng.")
            pq.write_table(pa.table({}), output_path, compression="snappy")
            return True

        # Cáº¯t theo ngÃ¢n sÃ¡ch náº¿u cáº§n
        if rows_budget is not None and len(first_chunk_raw) > rows_budget:
            first_chunk_raw = first_chunk_raw.iloc[:rows_budget]

        first_chunk_df = normalize_dataframe(first_chunk_raw)
        first_table = pa.Table.from_pandas(first_chunk_df, preserve_index=False)
        schema = first_table.schema

        print(f"ğŸ“‹ Detected schema with {len(schema.names)} columns")
        print(f"Columns: {schema.names}")

        writer = pq.ParquetWriter(output_path, schema=schema, compression="snappy")

        # Ghi chunk Ä‘áº§u tiÃªn
        writer.write_table(first_table)
        total_rows_in = len(first_chunk_raw)
        total_rows_written = len(first_chunk_df)
        total_rows_dropped = total_rows_in - total_rows_written

        # Trá»« ngÃ¢n sÃ¡ch
        if rows_budget is not None:
            rows_budget -= total_rows_written
            if rows_budget <= 0:
                writer.close()
                print("â›” Reached max rows limit; stopping.")
                print(f"ğŸ“Š Total written rows: {total_rows_written:,}")
                return True

        print(f"ğŸ”¢ Processed {total_rows_written:,}/{total_rows_in:,} rows (chunk 1)")

        # CÃ¡c chunk tiáº¿p theo
        chunk_idx = 2
        for raw_df in reader:
            total_rows_in += len(raw_df)

            # Náº¿u Ä‘Ã£ háº¿t ngÃ¢n sÃ¡ch thÃ¬ dá»«ng
            if rows_budget is not None and rows_budget <= 0:
                print("â›” Reached max rows limit; stopping.")
                break

            # Cáº¯t theo ngÃ¢n sÃ¡ch cÃ²n láº¡i
            if rows_budget is not None and len(raw_df) > rows_budget:
                raw_df = raw_df.iloc[:rows_budget]

            df = coerce_to_schema(raw_df, schema)
            written = bisect_write(writer, df, schema)
            dropped_here = len(df) - written
            total_rows_written += written
            total_rows_dropped += dropped_here

            if rows_budget is not None:
                rows_budget -= written

            info = f"ğŸ”¢ Processed {total_rows_written:,}/{total_rows_in:,} rows (chunk {chunk_idx})"
            if dropped_here > 0:
                info += f" â€” skipped {dropped_here:,} bad row(s)"
            print(info)
            chunk_idx += 1

            if rows_budget is not None and rows_budget <= 0:
                print("â›” Reached max rows limit; stopping.")
                break

        writer.close()

        print("âœ… Conversion finished.")
        print(f"ğŸ“Š Total input rows (seen): {total_rows_in:,}")
        print(f"ğŸ“ˆ Total written rows:      {total_rows_written:,}")
        print(f"ğŸ§¹ Total skipped rows:      {total_rows_dropped:,}")

        if Path(output_path).exists():
            size = Path(output_path).stat().st_size
            print(f"âœ… Output file: {output_path} ({size:,} bytes)")
            return True
        else:
            print(f"âŒ Output file was not created: {output_path}")
            return False

    except Exception as e:
        print(f"âŒ Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        return False
# --------------------- CLI ---------------------

def main():
    parser = argparse.ArgumentParser(
        description="Convert CSV.gz files to Parquet format using chunked processing (skips bad rows)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python convert_csv_to_parquet_chunked.py data.csv.gz data.parquet
  python convert_csv_to_parquet_chunked.py --input data.csv.gz --output data.parquet
  python convert_csv_to_parquet_chunked.py --input data.csv.gz --output data.parquet --chunk-size 25000
        """
    )
    parser.add_argument("input_file", nargs="?", help="Path to input CSV.gz file")
    parser.add_argument("output_file", nargs="?", help="Path to output Parquet file")
    parser.add_argument("--input", "-i", help="Path to input CSV.gz file (alternative to positional argument)")
    parser.add_argument("--output", "-o", help="Path to output Parquet file (alternative to positional argument)")
    parser.add_argument("--chunk-size", "-c", type=int, default=50000, help="Rows per chunk (default: 50000)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose output")
    parser.add_argument("--max-rows", "-m", type=int, default=None, help="Maximum number of rows to convert (default: all rows)")
    args = parser.parse_args()

    input_path = args.input or args.input_file
    output_path = args.output or args.output_file

    if not input_path:
        print("âŒ Error: Input file path is required")
        parser.print_help()
        sys.exit(1)
    if not output_path:
        print("âŒ Error: Output file path is required")
        parser.print_help()
        sys.exit(1)

    input_path = os.path.abspath(input_path)
    output_path = os.path.abspath(output_path)

    if args.verbose:
        print(f"Input file: {input_path}")
        print(f"Output file: {output_path}")
        print(f"Chunk size: {args.chunk_size}")

    success = convert_csv_to_parquet_chunked(input_path, output_path, args.chunk_size)
    if success:
        print("ğŸ‰ Conversion completed successfully!")
        sys.exit(0)
    else:
        print("ğŸ’¥ Conversion failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()
