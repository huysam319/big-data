version: "3.8"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
    ports:
      - "9870:9870" # Web UI HDFS
      - "8020:8020" # RPC
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./hadoop_config:/opt/hadoop/conf
    env_file:
      - ./.env
    networks:
      - bigdata
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - SERVICE_PRECONDITION=namenode:9870
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ./hadoop_config:/opt/hadoop/conf
    env_file:
      - ./.env
    networks:
      - bigdata
    depends_on:
      namenode:
        condition: service_healthy

  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
      - HOME=/root
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - SPARK_MASTER_WEBUI_PORT=8080
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.master.Master
      - --host
      - spark-master
      - --port
      - "7077"
      - --webui-port
      - "8080"
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./ivy2:/root/.ivy2
      - ./hadoop_config:/opt/hadoop/conf
      - ./scripts:/opt/scripts
      - ./data:/opt/data
    networks:
      - bigdata
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    environment:
      - SPARK_NO_DAEMONIZE=true
      - HOME=/root
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - spark://spark-master:7077
      - --webui-port
      - "8081"
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8081:8081"
    volumes:
      - ./ivy2:/root/.ivy2
      - ./hadoop_config:/opt/hadoop/conf
      - ./data:/opt/data
    networks:
      - bigdata
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  spark-thrift:
    image: apache/spark:3.5.0
    container_name: spark-thrift
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_NO_DAEMONIZE=true
      - HADOOP_CONF_DIR=/opt/hadoop/conf
    command: >
      bash -c "
      /opt/spark/sbin/start-thriftserver.sh
      --master spark://spark-master:7077
      --conf spark.sql.hive.thriftServer.singleSession=true
      --hiveconf hive.server2.thrift.port=10000
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0
      --conf spark.sql.warehouse.dir=hdfs://namenode:8020/user/hive/warehouse"
    ports:
      - "10000:10000"
    networks:
      - bigdata
    depends_on:
      spark-master:
        condition: service_healthy
      namenode:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - bigdata
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - bigdata
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "kafka-topics",
          "--list",
          "--bootstrap-server",
          "localhost:29092",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres_db:
    image: postgres:latest
    container_name: postgres_container
    environment:
      POSTGRES_USER: username
      POSTGRES_PASSWORD: password
      POSTGRES_DB: database
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - bigdata
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U username -d database"]
      interval: 30s
      timeout: 10s
      retries: 3

  grafana:
    image: grafana/grafana:10.4.5
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      # Đổi thông tin đăng nhập mặc định cho an toàn
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      # Tắt analytics gửi ra ngoài (tùy chọn)
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=hamedkarbasi93-kafka-datasource
    volumes:
      # Lưu trữ dữ liệu Grafana (users, dashboards đã import…)
      - grafana_storage:/var/lib/grafana
      # Tự động khai báo datasource/dashboards từ thư mục dự án
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - bigdata
    depends_on:
      postgres_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  hadoop_namenode:
  hadoop_datanode:
  postgres_data:
  grafana_storage:

networks:
  bigdata:
    driver: bridge
