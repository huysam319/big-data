{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "import os\n",
        "from pyspark.sql.functions import countDistinct\n",
        "from pyspark.sql.functions import col, count, countDistinct, sum, when, lit, concat_ws, collect_list, size, desc, asc\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MIMIC_Medication_Recommender_Optimized\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:8020\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.memoryFraction\", \"0.8\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.default.parallelism\", \"8\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .config(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"64MB\") \\\n",
        "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"=== Spark session initialized ===\")\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Master: {spark.conf.get('spark.master')}\")\n",
        "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")\n",
        "print(f\"Executor memory: {spark.conf.get('spark.executor.memory')}\")\n",
        "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Parquet files from HDFS (much more efficient than CSV)\n",
        "hdfs_uri = \"hdfs://namenode:8020\"\n",
        "\n",
        "print(\"Reading Parquet files from HDFS...\")\n",
        "\n",
        "# Read Parquet files (much more efficient than CSV)\n",
        "diagnoses_df = spark.read.parquet(hdfs_uri + \"/data/diagnoses_icd_part*.parquet\")\n",
        "prescriptions_df = spark.read.parquet(hdfs_uri + \"/data/prescriptions_part*.parquet\")\n",
        "icd_descriptions_df = spark.read.parquet(hdfs_uri + \"/data/d_icd_diagnoses_part*.parquet\")\n",
        "patients_df = spark.read.parquet(hdfs_uri + \"/data/patients_part*.parquet\")\n",
        "\n",
        "print(\"✓ All Parquet files loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized Data Processing with Memory Management\n",
        "print(\"=== OPTIMIZED DATA PROCESSING ===\")\n",
        "\n",
        "# Cache frequently used DataFrames for better performance\n",
        "print(\"Caching DataFrames for better performance...\")\n",
        "diagnoses_df.cache()\n",
        "prescriptions_df.cache()\n",
        "icd_descriptions_df.cache()\n",
        "patients_df.cache()\n",
        "\n",
        "# Get basic statistics\n",
        "print(f\"Diagnoses records: {diagnoses_df.count():,}\")\n",
        "print(f\"Prescriptions records: {prescriptions_df.count():,}\")\n",
        "print(f\"ICD descriptions: {icd_descriptions_df.count():,}\")\n",
        "print(f\"Patients: {patients_df.count():,}\")\n",
        "\n",
        "# Clean data with better memory management\n",
        "print(\"\\nCleaning data...\")\n",
        "diagnoses_clean = diagnoses_df.filter(col(\"icd_code\").isNotNull() & col(\"hadm_id\").isNotNull())\n",
        "prescriptions_clean = prescriptions_df.filter(col(\"drug\").isNotNull() & col(\"hadm_id\").isNotNull())\n",
        "\n",
        "# Repartition for better performance\n",
        "diagnoses_clean = diagnoses_clean.repartition(8, \"hadm_id\")\n",
        "prescriptions_clean = prescriptions_clean.repartition(8, \"hadm_id\")\n",
        "\n",
        "print(\"✓ Data cleaned and optimized for processing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized Join and Aggregation with Memory Management\n",
        "print(\"=== OPTIMIZED JOIN AND AGGREGATION ===\")\n",
        "\n",
        "# Join data with optimized strategy\n",
        "print(\"Performing optimized joins...\")\n",
        "diagnosis_med_join = diagnoses_clean.alias(\"d\").join(\n",
        "    prescriptions_clean.alias(\"p\"), \n",
        "    \"hadm_id\", \n",
        "    \"inner\"\n",
        ").join(\n",
        "    icd_descriptions_df.alias(\"desc\"),\n",
        "    [\"icd_code\", \"icd_version\"],\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# Repartition the joined data for better performance\n",
        "diagnosis_med_join = diagnosis_med_join.repartition(16, \"icd_code\")\n",
        "\n",
        "# Optimized aggregation with better memory management\n",
        "print(\"Performing optimized aggregation...\")\n",
        "medication_frequency = diagnosis_med_join.groupBy(\n",
        "    col(\"d.icd_code\"),\n",
        "    col(\"desc.long_title\").alias(\"diagnosis_description\"),\n",
        "    col(\"p.drug\")\n",
        ").agg(\n",
        "    count(\"*\").alias(\"prescription_count\"),\n",
        "    countDistinct(\"d.hadm_id\").alias(\"unique_patients_count\")\n",
        ")\n",
        "\n",
        "# Cache the aggregated result\n",
        "medication_frequency.cache()\n",
        "\n",
        "print(\"✓ Join and aggregation completed successfully\")\n",
        "\n",
        "# Thêm ranking cho mỗi diagnosis\n",
        "window_spec = Window.partitionBy(\"icd_code\").orderBy(col(\"prescription_count\").desc())\n",
        "\n",
        "medication_ranking = medication_frequency.withColumn(\n",
        "    \"rank\", \n",
        "    row_number().over(window_spec)\n",
        ")\n",
        "\n",
        "# Lấy top 5 thuốc cho mỗi chẩn đoán\n",
        "top_medications_per_diagnosis = medication_ranking.filter(col(\"rank\") <= 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized Recommendation Table Creation\n",
        "print(\"=== CREATING RECOMMENDATION TABLE ===\")\n",
        "\n",
        "# Add ranking for each diagnosis with optimized window function\n",
        "print(\"Adding rankings...\")\n",
        "window_spec = Window.partitionBy(\"icd_code\").orderBy(col(\"prescription_count\").desc())\n",
        "\n",
        "medication_ranking = medication_frequency.withColumn(\n",
        "    \"rank\", \n",
        "    row_number().over(window_spec)\n",
        ")\n",
        "\n",
        "# Get top 5 medications for each diagnosis\n",
        "top_medications_per_diagnosis = medication_ranking.filter(col(\"rank\") <= 5)\n",
        "\n",
        "# Create recommendation table with optimized calculations\n",
        "print(\"Creating final recommendation table...\")\n",
        "recommendation_table = top_medications_per_diagnosis.select(\n",
        "    col(\"icd_code\"),\n",
        "    col(\"diagnosis_description\"),\n",
        "    col(\"drug\"),\n",
        "    col(\"prescription_count\"),\n",
        "    col(\"unique_patients_count\"),\n",
        "    col(\"rank\"),\n",
        "    format_string(\n",
        "        \"%.2f\", \n",
        "        col(\"prescription_count\") * 100.0 / sum(\"prescription_count\").over(Window.partitionBy(\"icd_code\"))\n",
        "    ).alias(\"percentage_within_diagnosis\")\n",
        ")\n",
        "\n",
        "# Add diagnosis statistics\n",
        "diagnosis_stats = medication_frequency.groupBy(\"icd_code\").agg(\n",
        "    count(\"drug\").alias(\"total_different_meds\"),\n",
        "    sum(\"prescription_count\").alias(\"total_prescriptions\")\n",
        ")\n",
        "\n",
        "final_recommendation_table = recommendation_table.join(\n",
        "    diagnosis_stats, \"icd_code\", \"left\"\n",
        ")\n",
        "\n",
        "# Cache the final result\n",
        "final_recommendation_table.cache()\n",
        "\n",
        "print(\"✓ Recommendation table created successfully\")\n",
        "print(f\"Total recommendations: {final_recommendation_table.count():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Results Efficiently\n",
        "print(\"=== SAVING RESULTS ===\")\n",
        "\n",
        "# Create output directory in HDFS\n",
        "hdfs_output_path = \"hdfs://namenode:8020/processed/medication_recommendations\"\n",
        "\n",
        "print(\"Saving recommendation table to HDFS...\")\n",
        "try:\n",
        "    # Save as Parquet with optimized settings\n",
        "    final_recommendation_table.write \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"compression\", \"snappy\") \\\n",
        "        .parquet(hdfs_output_path)\n",
        "    \n",
        "    print(\"✓ Successfully saved to HDFS!\")\n",
        "    print(f\"Output path: {hdfs_output_path}\")\n",
        "    \n",
        "    # Show sample results\n",
        "    print(\"\\n=== SAMPLE RECOMMENDATIONS ===\")\n",
        "    final_recommendation_table.show(20, truncate=False)\n",
        "    \n",
        "    # Show summary statistics\n",
        "    print(\"\\n=== SUMMARY STATISTICS ===\")\n",
        "    print(f\"Total unique diagnoses: {final_recommendation_table.select('icd_code').distinct().count():,}\")\n",
        "    print(f\"Total unique medications: {final_recommendation_table.select('drug').distinct().count():,}\")\n",
        "    print(f\"Average medications per diagnosis: {final_recommendation_table.groupBy('icd_code').count().agg({'count': 'avg'}).collect()[0][0]:.1f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error saving results: {e}\")\n",
        "    print(\"Trying alternative save method...\")\n",
        "    \n",
        "    # Alternative: Save as CSV if Parquet fails\n",
        "    try:\n",
        "        final_recommendation_table.write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .csv(hdfs_output_path + \"_csv\")\n",
        "        print(\"✓ Successfully saved as CSV!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Failed to save results: {e2}\")\n",
        "\n",
        "print(\"\\n=== PROCESSING COMPLETED ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up and close Spark session\n",
        "print(\"=== CLEANING UP ===\")\n",
        "\n",
        "# Unpersist cached DataFrames to free memory\n",
        "try:\n",
        "    diagnoses_df.unpersist()\n",
        "    prescriptions_df.unpersist()\n",
        "    icd_descriptions_df.unpersist()\n",
        "    patients_df.unpersist()\n",
        "    medication_frequency.unpersist()\n",
        "    final_recommendation_table.unpersist()\n",
        "    print(\"✓ Cached DataFrames cleared\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"✓ Spark session stopped\")\n",
        "print(\"=== NOTEBOOK EXECUTION COMPLETED ===\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
